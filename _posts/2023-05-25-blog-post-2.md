---
title: 'How to evaluate ML models with geospatial data?'
date: 2023-05-25
permalink: /posts/2023/05/blog-post-2/
tags:
  - cool posts
  - geospatial
  - cross-validation
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool.

How to evaluate ML models with geospatial data?
<!-- ======

You can have many headings
======

Aren't headings cool?
------ -->

K-fold Cross-validation (KFCV) randomly divides a training set into K non-overlapping folds and iteratively holds out one fold at a time, training a model on the remainder (i.e., training folds) and measuring error on the held-out fold (i.e., validation fold). The average of these model errors across folds is the estimate of generalization performance on a test set.
KFCV provides unbiased estimates with independent, identically distributed (iid) data. But does it also work well on geospatial data?

Geospatial learning problems are frequently characterized by spatial autocorrelation in the input features and the potential for covariate shift at test time. These realities violate the iid assumption.
